[Messages]
out_nan = There was (a) NaN(s) in outputs
out_inf = There was (a) +/-Inf(s) in outputs
out_cons = Outputs of the model are constantly unchanged
lack_of_magnitude_express = The model seems to be unable to predict enough high magnitude values
lack_of_negative_express =  The model seems to be unable to predict negative values
w_nan = There was (a) NaN(s) in weight {}
w_inf = There was (a) +/-Inf(s) in weight {}
act_nan = There was (a) NaN(s) in activation {}
act_inf = There was (a) +/-Inf(s) in activation {}
conv_w_sign =  Conv. layer weight {} has {} as ratio of negative elements (>{}) 
fc_w_sign = FC layer weight {} has {} as ratio of negative elements (>{}) 
conv_w_dead = Conv. Layer weight {} has {} as ratio of dead elements (<={})
fc_w_dead = FC layer weight {} has {} as ratio of dead elements (<={})
conv_w_div_1 = Conv. layer weight are diverging until reaching abs mean of {} > {}
fc_w_div_1 = FC layer weight are diverging until reaching abs mean of {} > {}
conv_w_div_2 = Conv. layer weight are diverging with growth rate reaching {} > {}
fc_w_div_2 = FC layer weight are diverging with growth rate reaching {} > {}
b_poor = poor initialization of bias {}
b_nan = There was (a) NaN(s) in bias {}
b_inf = There was (a) +/-Inf(s) in bias : {}
b_div_1 = Bias {} is diverging until reaching abs mean of {} > {}
b_div_2 = Bias {} is diverging with growth rate reaching {} > {}
act_ltn = Activations of {} had values less than {}
act_gtn = Activations of {} had values greater than {}
conv_act_unstable = Conv. layer activation {} are considered unstable with std of {} far from [{}, {}]
fc_act_unstable = FC layer activation {} are considered unstable with std of {} far from [{}, {}]
conv_act_dead = {}/{} filter(s) of Conv. layer activation {} are considered dead
fc_act_dead = {}/{} neuron(s) of FC layer activation {} are considered dead
conv_act_sat ={}/{} filter(s) of Conv. layer activation {} are considered saturated
fc_act_sat = {}/{} neuron(s) of FC layer activation {} are considered saturated
grad_err = Gradient w.r.t weight {} seems to be incorrect with numerical error of {} > {}
gw_nan = There was (a) NaN(s) in gradient w.r.t weight {}
gw_inf = There was (a) +/-Inf(s) in gradient w.r.t weight {}
conv_gw_van_2 = Gradient w.r.t weight of Conv. layer {} are vanishing with decay rate reaching {} < {}
fc_gw_van_2 = Gradient w.r.t weight of FC layer {} are vanishing with decay rate reaching {} < {}
conv_gw_van_1 = Gradient w.r.t weight of Conv. layer {} are vanishing with reaching abs mean of {} < {}
fc_gw_van_1 = Gradient w.r.t weight of FC layer {} are vanishing with reaching abs mean of {} < {}
conv_gw_exp_2 = Gradient w.r.t weight of Conv. layer {} are exploding with growth rate reaching {} > {} 
fc_gw_exp_2 = Gradient w.r.t weight of FC layer {} are exploding with growth rate reaching {} > {}
conv_gw_exp_1 = Gradient w.r.t weight of Conv. layer {} are exploding with reaching abs mean of {} > {}
fc_gw_exp_1 = Gradient w.r.t weight of FC layer {} are exploding with reaching abs mean of {} > {}
w_untrained = The weights of {} are untrained (no value is changing) 
conv_w_fast = Conv. layer weight {} change too quickly with magnitude update ratio of {} > {}
fc_w_fast = FC layer weight {} change too quickly with magnitude update ratio of {} > {}
conv_w_slow = Conv. layer weight {} change too slowly with magnitude update ratio of {} < {}
fc_w_slow = FC layer weight of {} change too slowly with magnitude update ratio of {} < {}
output_invalid = The predicted outputs are not valid with respect to the target
nan_loss = Loss is turning into NaN
inf_loss = Loss is turning into +/-Inf
div_loss = Loss is diverging with absolute rate reaching {}
poor_init_loss = Loss at cold start is considered poor and problematic: relative error of {}
fluctuated_loss = There is a lot of fluctuations, the smoothness of Loss is {} > {}
poor_reduction_loss = The reduction of the loss is poorly designed: it is recommended to use AVG instead
stagnated_loss = The loss is no-or-slowly decreasing
repr_loss = The loss is not representative for real risk of the system with prop of {}
features_constant = Features are constant
feature_constant = Feature indexed {} is constant
targets_constant = Outputs are constant
target_constant = Output indexed {} is constant
features_unnormalized = Features seem to be unnormalized
feature_unnormalized = Feature indexed {} seems to be unnormalized
targets_unnormalized = Outputs seem to be unnormalized
target_unnormalized = Output indexed {} seem to be unnormalized
unbalanced_labels = Groundtruth Labels are unbalanced, which requires adaptive algorithms
need_he = It is recommended to choose He initialization for weight {} with RELU: error of {}
need_glorot = It is recommended to choose Glorot/Xavier initialization for weight {} with Tanh: error of {}
need_lecun = It is recommended to choose Lecun initialization for weight {} with Sigmoid: error of {}
need_init_well = It is recommended to choose a well-known initialization for weight {}
poor_init = Poor initialization (unbreaking symmetry) of weight {}
need_bias = The model missed biases. Do not consider this alert if you use batchnorm for all layers
last_bias = Bias of last layer should not be zero, in case of unbalanced data
ineff_bias_cls = Bias of last layer should match the ratio of labels
ineff_bias_regr = Bias of last layer should start up with the mean value
zero_bias = It is recommended to choose null biases (zeros)
Data_dep = The training procedure seems to be not considering the data inputs
op_dep = The operation {} has no appropriate dependancy with the input data
overwhelm_reg_loss = The data loss is overwhelmed by regularization losses reaching {} > {}.
zero_loss = The loss is smoothly decreasing towards zero: The model may need regularization
corrupted_data = The data is likely to be very noisy or corrupted
underfitting_single_batch = The DNN training is unable to fit properly a single batch of data
non_representative_loss = The loss seems to lack representativeness: the correlation magnitude between loss and performance measures equals to {} < {}
switch_mode_act_consistency = After switch from train to eval mode, hidden activations have low consistency with similarity of {} < {}
switch_mode_out_consistency = After switch from train to eval mode, outputs have low consistency with similarity of {} < {}
switch_mode_loss_consistency = After switch from train to eval mode, the loss is no longer consistent (train mode:{} =/= eval mode:{}) 
wrong_augm = The augmentation of data induced a shift in the data distribution