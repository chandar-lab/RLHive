Observations:
    Period: 0
    Data:
        normalized_data_mins: [0.0, -1.0]
        normalized_data_maxs: [1.0]
        #normalized perplexity of labels,
        #where 0-->a single label overcomes the dist, 1--> all the labels have almost equal frequencies.
        labels_perp_min_thresh: 0.5
        outputs_var_coef_thresh: 0.001

Weights:
    Period: 0
    Initial_Weight:
        disabled: false
        f_test_alpha: 0.1

PreCheck:
    fail_on: false
    disabled: false
    Data:
        normalized_data_mins: [0.0, -1.0]
        normalized_data_maxs: [1.0]
        #normalized perplexity of labels, 
        #where 0-->a single label overcomes the dist, 1--> all the labels have almost equal frequencies.
        labels_perp_min_thresh: 0.5
        outputs_var_coef_thresh: 0.001 
    Initial_Weight:
        disabled: false
        f_test_alpha: 0.1
    Initial_Bias:
        disabled: false
    Initial_Loss:
        #size of data sample will be used to check the initial loss given random parameters.
        sample_size: 128
        data_size_growth_rate: 2
        data_size_growth_iters: 5
        dev_ratio: 1.0
    Gradient:
        disabled: true
        #warm up learning steps to avoid checking at cold start situation.
        warm_up_steps: 10
        warm_up_batch: 32
        #number of instances on which all the gradients are numerically computed for verification
        sample_size: 3 
        #Ratio of randomly selected dimensions (0.1 means 10%) would be used for gradient checking
        ratio_of_dimensions: 0.1 
        delta: 0.0001 #(1e-4) refers to numerical gradient checking formula
        relative_err_max_thresh: 0.01 #refers to numerical gradient checking formula
    Proper_Fitting:
        single_batch_size: 16
        total_iters: 1000
        abs_loss_min_thresh: 1e-8
        loss_min_thresh: 0.00001 # (1e-5) check of close-to-zero loss (lack of regularization)
        smoothness_max_thresh: 0.95 # check of very smoothly decreasing loss (lack of regularization)
        mislabeled_rate_max_thresh: 0.05 #no more than 5% error rate w.r.t a single batch of data
        mean_error_max_thresh: 0.001 #max of the mean of error per instance (assuming output is normalized within [0,1])
        #refers to number of loss values used to perform statistical comparison between two models:
        #one takes the features as inputs and one takes zeroed features(null) to test for data dependency 
        sample_size_of_losses: 100
    Instance_wise_Operation:
        sample_size: 32
        trials: 10

OverfitCheck:
    fail_on: false
    disabled: false
    start: 10
    period: 10
    patience: 5
    regr_perf_thresh: 0.0001
    classif_perf_thresh: 1.0
    Activation:
        Dead:
            act_min_thresh: 0.00001
            act_maj_percentile: 95
            neurons_ratio_max_thresh: 0.5
        Saturation:
            ro_histo_bins_count: 50
            ro_histo_min: 0.0
            ro_histo_max: 1.0
            ro_max_thresh: 0.85
            neurons_ratio_max_thresh: 0.5
        Distribution:
            std_acts_min_thresh: 0.5
            std_acts_max_thresh: 2.0
            f_test_alpha: 0.1
        Range:
            disabled: true
        Output:
            patience: 5
        Numerical_Instability:
            disabled: false
    Weight:
        Dead:
            # the value of weight below which it would considered dead
            value_min_thresh: 0.00001 #1e-5
            # the maximum of dead weights ratio by layer to consider the layer is dead
            ratio_max_thresh: 0.95
        Negative:
            # the maximum of negative weights ratio by layer to warn the user
            ratio_max_thresh: 0.95
        Diverging:
            # it refers to the time periods for which we perform the growth scale checking
            window_size: 5
            # it refers to the maximum of mean absolute value(mav) for weights to make sense. 
            mav_max_thresh: 100000000 #1e8
            # it refers to the maximum of growth rate (r) to detect exponential growth tendency
            inc_rate_max_thresh: 2
        Numerical_Instability:
            disabled: false
    Bias:
        Diverging:
            window_size: 5
            mav_max_thresh: 100000000
            inc_rate_max_thresh: 2
        Numerical_Instability:
            disabled: false
    Gradient:
        Vanishing:
            window_size: 5
            mav_min_thresh: 0.000001
            dec_rate_min_thresh: 0.5
        Exploding:
            window_size: 5
            mav_max_thresh: 1000000
            inc_rate_max_thresh: 2
        Unstable_Learning:
            # the following refer to high and low log_10 scale of mean absolute values of updates 
            high_updates_max_thresh: -1
            low_updates_min_thresh: -4
        Numerical_Instability:
            disabled: false
    Loss:
        NonDecreasing:
            window_size: 5
            # it means the loss should decrease by at least 5% of its value for each period of check
            decr_percentage: 0.05
        Diverging:
            window_size: 5
            # it means recent losses is increasing by rate of >= 2 w.r.t the minimum reached loss.
            incr_abs_rate_max_thresh: 2
        Fluctuating:
            window_size: 50
            # it means no more than 50% ratio of the incr/decr switches (sign changes of derivatives) w.r.t to the estimated losses (data points)
            smoothness_ratio_min_thresh: 0.5
        Representativeness:
            # it means the absolute coeff of correlation should not go below 0.5 between performance metrics
            abs_corr_min_thresh: 0.5
        Overwhelming_Reg:
            window_size: 5
            # it refers to the growth rate of the ratio of reg loss gradient norm by data loss gradient norm.
            growth_rate_max_thresh: 2
        Numerical_Instability:
            disabled: false

PostCheck:
    fail_on: false
    disabled: false
    start: 10
    period: 250
    Corrupted_Labels:
        disabled: false
        batch_size: 32
        warmup_epochs: 5
        total_epochs: 50    
        patience: 5
        perf_improv_ratio_min_thresh: 0.1
    Data_Augm:
        disabled: false
        batch_size: 64
        total_epochs: 50
        valid_sample_size: 1024
        # aims to spot substantial changes in the original activations between enable/disable augmentation
        sim_with_augm_min_thresh: 0.95
    # aims to spot abnormal differences of activations/logits when switching from train to test mode
    Switch_Mode_Consistency:
        disabled: false
        batch_size: 64
        total_epochs: 50
        valid_sample_size: 1024
        # regarding similarity w.r.t activations and outputs
        sim_after_switch_mode_min_thresh: 0.75
        # regarding relative loss difference
        relative_loss_diff_max_thresh: 0.5 #means 50% of growth or decay of loss value when switching modes.